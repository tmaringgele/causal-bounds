{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c504e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# user defined R installation\n",
    "import os\n",
    "os.environ['R_HOME'] = 'D:/Program Files/R-4.5.0' #path to your R installation\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import globalenv\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.vectors import FloatVector\n",
    "\n",
    "from causaloptim_python import run_experiment, extract_prob_dict\n",
    "\n",
    "\n",
    "\n",
    "# Load R packages\n",
    "#igraph = importr('igraph')\n",
    "causaloptim = importr('causaloptim')\n",
    "base = importr('base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828535c2",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE: 0.19831845457157393\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0893491a",
   "metadata": {},
   "source": [
    "#### causaloptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE: 0.19725243674534862\n",
      "Bounds: (<rpy2.robjects.vectors.FloatVector object at 0x000001762E758510> [14]\n",
      "R classes: ('numeric',)\n",
      "[-0.117957], <rpy2.robjects.vectors.FloatVector object at 0x000001762E727B10> [14]\n",
      "R classes: ('numeric',)\n",
      "[0.545062])\n"
     ]
    }
   ],
   "source": [
    "# Set seed and generate synthetic data in Python\n",
    "np.random.seed(299128)\n",
    "n = 5000\n",
    "Z = np.random.binomial(1, 0.5, size=n)\n",
    "U = np.random.binomial(1, 0.5, size=n)\n",
    "\n",
    "# X ~ Z + U\n",
    "logit_X = -1 + 1.5 * Z + 1.2 * U\n",
    "p_X = 1 / (1 + np.exp(-logit_X))\n",
    "X = np.random.binomial(1, p_X)\n",
    "\n",
    "# Y ~ X + U\n",
    "beta_X, beta_U = 1.0, 1.5\n",
    "logit_Y = -0.5 + beta_X * X + beta_U * U\n",
    "p_Y = 1 / (1 + np.exp(-logit_Y))\n",
    "Y = np.random.binomial(1, p_Y)\n",
    "\n",
    "df = pd.DataFrame({'Y': Y, 'X': X, 'Z': Z})\n",
    "\n",
    "# Ground truth ATE\n",
    "logit_Y1 = -0.5 + beta_X * 1 + beta_U * U\n",
    "logit_Y0 = -0.5 + beta_X * 0 + beta_U * U\n",
    "p_Y1 = 1 / (1 + np.exp(-logit_Y1))\n",
    "p_Y0 = 1 / (1 + np.exp(-logit_Y0))\n",
    "ATE_true = np.mean(p_Y1 - p_Y0)\n",
    "print(\"True ATE:\", ATE_true)\n",
    "\n",
    "\n",
    "graph_str = \"(Z -+ X, X -+ Y, Ur -+ X, Ur -+ Y)\"\n",
    "leftside = [1, 0, 0, 0]\n",
    "latent   = [0, 0, 0, 1]\n",
    "nvals    = [2, 2, 2, 2]\n",
    "rlconnect = [0, 0, 0, 0]\n",
    "monotone = [0, 0, 0, 0]\n",
    "\n",
    "prob_dict = extract_prob_dict(df)\n",
    "\n",
    "\n",
    "bounds = run_experiment(graph_str, leftside, latent, nvals, rlconnect, monotone, prob_dict)\n",
    "print(\"Bounds:\", bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a30d85",
   "metadata": {},
   "source": [
    "#### Varying Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf6585",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 124\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Bin \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbins[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbins[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbin_counts[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m--> 124\u001b[0m sim_data \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_uniform_ate_distribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples_per_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimulate_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnoise_std\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# more variation!\u001b[39;49;00m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    130\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m df_uniform \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sim_data)\n\u001b[0;32m    133\u001b[0m df_uniform\u001b[38;5;241m.\u001b[39mATE_true\u001b[38;5;241m.\u001b[39mhist(bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[217], line 103\u001b[0m, in \u001b[0;36msimulate_uniform_ate_distribution\u001b[1;34m(n, n_bins, samples_per_bin, bin_range, simulate_kwargs, max_attempts, verbose)\u001b[0m\n\u001b[0;32m    100\u001b[0m total_required \u001b[38;5;241m=\u001b[39m n_bins \u001b[38;5;241m*\u001b[39m samples_per_bin\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28msum\u001b[39m(bin_counts) \u001b[38;5;241m<\u001b[39m total_required \u001b[38;5;129;01mand\u001b[39;00m attempts \u001b[38;5;241m<\u001b[39m max_attempts:\n\u001b[1;32m--> 103\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_with_target_ate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_ate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_range\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_range\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msimulate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     ate \u001b[38;5;241m=\u001b[39m sim[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATE_true\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    105\u001b[0m     bin_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(ate, bins) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[217], line 56\u001b[0m, in \u001b[0;36msimulate_with_target_ate\u001b[1;34m(n, min_ate, max_ate, max_attempts, coef_ranges, intercept_ranges, noise_std)\u001b[0m\n\u001b[0;32m     54\u001b[0m logit_Y \u001b[38;5;241m=\u001b[39m intercept_Y \u001b[38;5;241m+\u001b[39m b_X_Y \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m b_U_Y \u001b[38;5;241m*\u001b[39m U \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, noise_std, size\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m     55\u001b[0m p_Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mlogit_Y))\n\u001b[1;32m---> 56\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_Y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Counterfactual outcomes with same noise structure\u001b[39;00m\n\u001b[0;32m     59\u001b[0m logit_Y1 \u001b[38;5;241m=\u001b[39m intercept_Y \u001b[38;5;241m+\u001b[39m b_X_Y \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m b_U_Y \u001b[38;5;241m*\u001b[39m U \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, noise_std, size\u001b[38;5;241m=\u001b[39mn)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def simulate_with_target_ate(\n",
    "    n=5000,\n",
    "    min_ate=0.05,\n",
    "    max_ate=0.95,\n",
    "    max_attempts=1000,\n",
    "    coef_ranges=None,\n",
    "    intercept_ranges=None,\n",
    "    noise_std=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulates data for binary treatment effect analysis.\n",
    "    Fixes clustering by:\n",
    "    - Using continuous U ~ N(0,1)\n",
    "    - Adding Gaussian noise to logits\n",
    "\n",
    "    Returns dict with true ATE and all coefficients.\n",
    "    \"\"\"\n",
    "    if coef_ranges is None:\n",
    "        coef_ranges = {\n",
    "            'b_Z': (0.0, 3.0),\n",
    "            'b_U_X': (0.0, 3.0),\n",
    "            'b_X_Y': (0.5, 6.0),\n",
    "            'b_U_Y': (0.0, 3.0)\n",
    "        }\n",
    "    if intercept_ranges is None:\n",
    "        intercept_ranges = {\n",
    "            'intercept_X': (-2.0, 2.0),\n",
    "            'intercept_Y': (-4.0, 2.0)\n",
    "        }\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        seed = np.random.randint(0, 1e6)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Coefficients\n",
    "        b_Z = np.random.uniform(*coef_ranges['b_Z'])\n",
    "        b_U_X = np.random.uniform(*coef_ranges['b_U_X'])\n",
    "        b_X_Y = np.random.uniform(*coef_ranges['b_X_Y'])\n",
    "        b_U_Y = np.random.uniform(*coef_ranges['b_U_Y'])\n",
    "\n",
    "        intercept_X = np.random.uniform(*intercept_ranges['intercept_X'])\n",
    "        intercept_Y = np.random.uniform(*intercept_ranges['intercept_Y'])\n",
    "\n",
    "        # Generate continuous latent confounder\n",
    "        Z = np.random.binomial(1, 0.5, size=n)\n",
    "        U = np.random.normal(0, 1, size=n)\n",
    "\n",
    "        # Generate treatment X\n",
    "        logit_X = intercept_X + b_Z * Z + b_U_X * U\n",
    "        p_X = 1 / (1 + np.exp(-logit_X))\n",
    "        X = np.random.binomial(1, p_X)\n",
    "\n",
    "        # Generate outcome Y with noise\n",
    "        logit_Y = intercept_Y + b_X_Y * X + b_U_Y * U + np.random.normal(0, noise_std, size=n)\n",
    "        p_Y = 1 / (1 + np.exp(-logit_Y))\n",
    "        Y = np.random.binomial(1, p_Y)\n",
    "\n",
    "        # Counterfactual outcomes with same noise structure\n",
    "        logit_Y1 = intercept_Y + b_X_Y * 1 + b_U_Y * U + np.random.normal(0, noise_std, size=n)\n",
    "        logit_Y0 = intercept_Y + b_X_Y * 0 + b_U_Y * U + np.random.normal(0, noise_std, size=n)\n",
    "        p_Y1 = 1 / (1 + np.exp(-logit_Y1))\n",
    "        p_Y0 = 1 / (1 + np.exp(-logit_Y0))\n",
    "        ATE_true = np.mean(p_Y1 - p_Y0)\n",
    "\n",
    "        if min_ate <= ATE_true <= max_ate:\n",
    "            return {\n",
    "                'seed': seed,\n",
    "                'b_Z': b_Z,\n",
    "                'b_U_X': b_U_X,\n",
    "                'b_X_Y': b_X_Y,\n",
    "                'b_U_Y': b_U_Y,\n",
    "                'intercept_X': intercept_X,\n",
    "                'intercept_Y': intercept_Y,\n",
    "                'ATE_true': ATE_true,\n",
    "                'Z': Z,\n",
    "                'U': U,\n",
    "                'X': X,\n",
    "                'Y': Y\n",
    "            }\n",
    "\n",
    "    raise ValueError(f\"Failed to generate ATE in range [{min_ate}, {max_ate}] after {max_attempts} attempts\")\n",
    "\n",
    "def simulate_uniform_ate_distribution(\n",
    "    n=5000,\n",
    "    n_bins=10,\n",
    "    samples_per_bin=50,\n",
    "    bin_range=(0.05, 0.95),\n",
    "    simulate_kwargs=None,\n",
    "    max_attempts=30000,\n",
    "    verbose=True\n",
    "):\n",
    "    if simulate_kwargs is None:\n",
    "        simulate_kwargs = {}\n",
    "\n",
    "    bins = np.linspace(*bin_range, n_bins + 1)\n",
    "    bin_counts = np.zeros(n_bins, dtype=int)\n",
    "    data = []\n",
    "\n",
    "    attempts = 0\n",
    "    total_required = n_bins * samples_per_bin\n",
    "\n",
    "    while sum(bin_counts) < total_required and attempts < max_attempts:\n",
    "        sim = simulate_with_target_ate(n=n, min_ate=bin_range[0], max_ate=bin_range[1], **simulate_kwargs)\n",
    "        ate = sim[\"ATE_true\"]\n",
    "        bin_idx = np.digitize(ate, bins) - 1\n",
    "\n",
    "        if 0 <= bin_idx < n_bins and bin_counts[bin_idx] < samples_per_bin:\n",
    "            bin_counts[bin_idx] += 1\n",
    "            data.append(sim)\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    if attempts >= max_attempts and sum(bin_counts) < total_required:\n",
    "        print(f\"⚠️ Warning: Only filled {sum(bin_counts)} / {total_required} samples after {attempts} attempts.\")\n",
    "        if verbose:\n",
    "            for i in range(n_bins):\n",
    "                print(f\"  Bin {i} ({bins[i]:.2f}-{bins[i+1]:.2f}): {bin_counts[i]} samples\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sim_data = simulate_uniform_ate_distribution(\n",
    "    n=5000,\n",
    "    n_bins=5,\n",
    "    samples_per_bin=25,\n",
    "    simulate_kwargs={'noise_std': 1.0},  # more variation!\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df_uniform = pd.DataFrame(sim_data)\n",
    "df_uniform.ATE_true.hist(bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4804594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "n = 50000\n",
    "graph_str = \"(Z -+ X, X -+ Y, Ur -+ X, Ur -+ Y)\"\n",
    "leftside = [1, 0, 0, 0]\n",
    "latent   = [0, 0, 0, 1]\n",
    "nvals    = [2, 2, 2, 2]\n",
    "rlconnect = [0, 0, 0, 0]\n",
    "monotone = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(500):\n",
    "    # Simulate data with diverse ATEs\n",
    "    sim = simulate_with_target_ate(n=n, min_ate=0.05, max_ate=0.95)\n",
    "\n",
    "    # Create dataframe and get probabilities\n",
    "    df = pd.DataFrame({'Y': sim['Y'], 'X': sim['X'], 'Z': sim['Z']})\n",
    "    prob_dict = extract_prob_dict(df)\n",
    "\n",
    "    # Run causaloptim\n",
    "    r_bounds = run_experiment(\n",
    "        graph_str, leftside, latent, nvals, rlconnect, monotone, prob_dict\n",
    "    )\n",
    "    bound_lower = float(r_bounds[0][0])\n",
    "    bound_upper = float(r_bounds[1][0])\n",
    "\n",
    "    # Save result\n",
    "    results.append({\n",
    "        'seed': sim['seed'],\n",
    "        'b_Z': sim['b_Z'],\n",
    "        'b_U_X': sim['b_U_X'],\n",
    "        'b_X_Y': sim['b_X_Y'],\n",
    "        'b_U_Y': sim['b_U_Y'],\n",
    "        'ATE_true': sim['ATE_true'],\n",
    "        'bound_lower': bound_lower,\n",
    "        'bound_upper': bound_upper\n",
    "    })\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "25ad317a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    500.000000\n",
       "mean       0.388889\n",
       "std        0.123048\n",
       "min        0.102664\n",
       "25%        0.320806\n",
       "50%        0.408023\n",
       "75%        0.473789\n",
       "max        0.602265\n",
       "Name: ATE_true, dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.ATE_true.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e4e21",
   "metadata": {},
   "source": [
    "#### Iteration testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "465ac0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 437838\n",
      "Coefficients: b_Z=0.677, b_U_X=1.496, b_X_Y=2.038, b_U_Y=0.409\n",
      "True ATE: 0.421055270147257\n",
      "Bounds: -0.172 — 0.673\n"
     ]
    }
   ],
   "source": [
    "def simulate_with_target_ate(n=5000, min_ate=0.05, max_ate=0.95, max_attempts=1000):\n",
    "    for attempt in range(max_attempts):\n",
    "        seed = np.random.randint(0, 1e6)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Random coefficients\n",
    "        b_Z = np.round(np.random.uniform(0.5, 2.0), 3)\n",
    "        b_U_X = np.round(np.random.uniform(0.5, 2.0), 3)\n",
    "        b_X_Y = np.round(np.random.uniform(0.5, 5.0), 3)   # wider range to stretch ATE\n",
    "        b_U_Y = np.round(np.random.uniform(0.0, 2.0), 3)\n",
    "\n",
    "        # Generate data\n",
    "        Z = np.random.binomial(1, 0.5, size=n)\n",
    "        U = np.random.binomial(1, 0.5, size=n)\n",
    "\n",
    "        logit_X = -1 + b_Z * Z + b_U_X * U\n",
    "        p_X = 1 / (1 + np.exp(-logit_X))\n",
    "        X = np.random.binomial(1, p_X)\n",
    "\n",
    "        logit_Y = -0.5 + b_X_Y * X + b_U_Y * U\n",
    "        p_Y = 1 / (1 + np.exp(-logit_Y))\n",
    "        Y = np.random.binomial(1, p_Y)\n",
    "\n",
    "        # Compute true ATE\n",
    "        logit_Y1 = -0.5 + b_X_Y * 1 + b_U_Y * U\n",
    "        logit_Y0 = -0.5 + b_X_Y * 0 + b_U_Y * U\n",
    "        p_Y1 = 1 / (1 + np.exp(-logit_Y1))\n",
    "        p_Y0 = 1 / (1 + np.exp(-logit_Y0))\n",
    "        ATE_true = np.mean(p_Y1 - p_Y0)\n",
    "\n",
    "        if min_ate <= ATE_true <= max_ate:\n",
    "            return {\n",
    "                'seed': seed,\n",
    "                'b_Z': b_Z,\n",
    "                'b_U_X': b_U_X,\n",
    "                'b_X_Y': b_X_Y,\n",
    "                'b_U_Y': b_U_Y,\n",
    "                'ATE_true': ATE_true,\n",
    "                'Z': Z,\n",
    "                'U': U,\n",
    "                'X': X,\n",
    "                'Y': Y\n",
    "            }\n",
    "\n",
    "    raise ValueError(f\"Failed to generate ATE in range [{min_ate}, {max_ate}] after {max_attempts} attempts\")\n",
    "\n",
    "\n",
    "sim = simulate_with_target_ate(n=5000, min_ate=0.05, max_ate=0.95)\n",
    "\n",
    "df = pd.DataFrame({'Y': sim['Y'], 'X': sim['X'], 'Z': sim['Z']})\n",
    "prob_dict = extract_prob_dict(df)\n",
    "r_bounds = run_experiment(\n",
    "    graph_str, leftside, latent, nvals, rlconnect, monotone, prob_dict\n",
    ")\n",
    "bound_lower = float(r_bounds[0][0])\n",
    "bound_upper = float(r_bounds[1][0])\n",
    "\n",
    "print(f\"Seed: {sim['seed']}\")\n",
    "print(f\"Coefficients: b_Z={sim['b_Z']}, b_U_X={sim['b_U_X']}, b_X_Y={sim['b_X_Y']}, b_U_Y={sim['b_U_Y']}\")\n",
    "print(f\"True ATE: {sim['ATE_true']}\")\n",
    "print(f\"Bounds: {bound_lower:.3f} — {bound_upper:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a00d7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 289586\n",
      "Coefficients:\n",
      "  b_Z: 0.908\n",
      "  b_U_X: 1.942\n",
      "  b_X_Y: 1.967\n",
      "  b_U_Y: 0.871\n",
      "True ATE: 0.37794317326984606\n",
      "Probabilities =1:\n",
      " p(Y=1): [0.91197567 0.91197567 0.37754067 ... 0.91197567 0.81260097 0.37754067]\n",
      " p(X=1): [0.8641271  0.8641271  0.47701621 ... 0.8641271  0.47701621 0.26894142]\n",
      "Bounds:\n",
      "  Lower: -0.1325433177613982\n",
      "  Upper: 0.7056929857810956\n"
     ]
    }
   ],
   "source": [
    "seed = np.random.randint(0, 1e6)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Random coefficients\n",
    "b_Z = np.round(np.random.uniform(0.5, 2.0), 3)\n",
    "b_U_X = np.round(np.random.uniform(0.5, 2.0), 3)\n",
    "b_X_Y = np.round(np.random.uniform(0.5, 2.0), 3)\n",
    "b_U_Y = np.round(np.random.uniform(0.5, 2.0), 3)\n",
    "\n",
    "# Simulate data\n",
    "Z = np.random.binomial(1, 0.5, size=n)\n",
    "U = np.random.binomial(1, 0.5, size=n)\n",
    "\n",
    "logit_X = -1 + b_Z * Z + b_U_X * U\n",
    "p_X = 1 / (1 + np.exp(-logit_X))\n",
    "X = np.random.binomial(1, p_X)\n",
    "\n",
    "logit_Y = -0.5 + b_X_Y * X + b_U_Y * U\n",
    "p_Y = 1 / (1 + np.exp(-logit_Y))\n",
    "Y = np.random.binomial(1, p_Y)\n",
    "\n",
    "# True ATE\n",
    "logit_Y1 = -0.5 + b_X_Y * 1 + b_U_Y * U\n",
    "logit_Y0 = -0.5 + b_X_Y * 0 + b_U_Y * U\n",
    "p_Y1 = 1 / (1 + np.exp(-logit_Y1))\n",
    "p_Y0 = 1 / (1 + np.exp(-logit_Y0))\n",
    "ATE_true = np.mean(p_Y1 - p_Y0)\n",
    "\n",
    "# Estimate bounds\n",
    "df = pd.DataFrame({'Y': Y, 'X': X, 'Z': Z})\n",
    "prob_dict = extract_prob_dict(df)\n",
    "r_bounds = run_experiment(\n",
    "graph_str, leftside, latent, nvals, rlconnect, monotone, prob_dict\n",
    ")\n",
    "bound_lower = float(r_bounds[0][0])\n",
    "bound_upper = float(r_bounds[1][0])\n",
    "\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"Coefficients:\")\n",
    "print(f\"  b_Z: {b_Z}\")\n",
    "print(f\"  b_U_X: {b_U_X}\")\n",
    "print(f\"  b_X_Y: {b_X_Y}\")\n",
    "print(f\"  b_U_Y: {b_U_Y}\")\n",
    "print(f\"True ATE: {ATE_true}\")\n",
    "print(f\"Probabilities =1:\")\n",
    "print(f\" p(Y=1): {p_Y}\")\n",
    "print(f\" p(X=1): {p_X}\")\n",
    "\n",
    "print(f\"Bounds:\")\n",
    "print(f\"  Lower: {bound_lower}\")\n",
    "print(f\"  Upper: {bound_upper}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
